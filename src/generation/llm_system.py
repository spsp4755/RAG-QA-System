import os
from typing import List, Dict, Optional
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from sentence_transformers import util


class LLMSystem:
    """ë¡œì»¬ LLM ê¸°ë°˜ ë‹µë³€ ìƒì„± ì‹œìŠ¤í…œ"""
    
    def __init__(self, model_name: str = "EleutherAI/polyglot-ko-1.3b"):
        """
        LLM ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        
        Args:
            model_name: ì‚¬ìš©í•  ëª¨ë¸ëª… (CPU í˜¸í™˜ ëª¨ë¸ ê¶Œì¥)
        """
        self.model_name = model_name
        self.device = "cpu"  # MacBook CPU ì‚¬ìš©
        
        print(f"ğŸ¤– LLM ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
        try:
            print(f"ğŸ“¥ í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ ì¤‘...")
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            
            print(f"ğŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘...")
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float32,  # CPU í˜¸í™˜
                low_cpu_mem_usage=True
            )
            
            # íŒ¨ë”© í† í° ì„¤ì •
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ë¬¸ì¥ ì¶”ì¶œìš©)
            from sentence_transformers import SentenceTransformer
            self.embedding_model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
                
            print("âœ… LLM ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ LLM ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            print("âš ï¸ SimpleLLMSystemìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            raise e
    
    def generate_answer(self, query: str, context_docs: List[Dict], max_new_tokens: int = 256) -> str:
        """
        ë¬¸ì¥ ì¶”ì¶œ ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ìƒì„± + ê°„ê²° í”„ë¡¬í”„íŠ¸
        """
        if not context_docs:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        try:
            context_text = self._build_context(context_docs, query)
            prompt = self._build_prompt(query, context_text)
            print(f"í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)} ë¬¸ì")
            inputs = self.tokenizer.encode(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=512
            )
            print(f"ì…ë ¥ í† í° ìˆ˜: {inputs.shape[1]}")
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_new_tokens=150,
                    num_return_sequences=1,
                    temperature=0.7,
                    do_sample=True,
                    top_p=0.9,
                    repetition_penalty=1.2,  # ë°˜ë³µ ë°©ì§€
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            answer = generated_text[len(prompt):].strip()
            print(f"ìƒì„±ëœ ë‹µë³€ ê¸¸ì´: {len(answer)} ë¬¸ì")
            if not answer:
                return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            return answer
        except Exception as e:
            print(f"LLM ìƒì„± ì˜¤ë¥˜: {e}")
            return f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    def _build_context(self, context_docs: List[Dict], question: str) -> str:
        # ë¬¸ì„œë³„ë¡œ ì§ˆë¬¸ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì¥ 1-2ê°œë§Œ ì¶”ì¶œ (ë” ì§§ê²Œ)
        context_parts = []
        for i, doc in enumerate(context_docs[:1], 1):  # ìµœëŒ€ 1ê°œ ë¬¸ì„œë§Œ ì‚¬ìš©
            doc_text = doc['text']
            # ë¬¸ì¥ ë¶„ë¦¬ (ê°„ë‹¨í•˜ê²Œ ë§ˆì¹¨í‘œ ê¸°ì¤€)
            sentences = [s.strip() for s in doc_text.split('.') if s.strip()]
            if not sentences:
                continue
            # ì„ë² ë”© ëª¨ë¸ ì¬ì‚¬ìš©
            q_emb = self.embedding_model.encode(question)
            s_embs = self.embedding_model.encode(sentences)
            sims = util.cos_sim(q_emb, s_embs)[0]
            top_indices = sims.argsort(descending=True)[:2]  # ìµœëŒ€ 2ê°œ ë¬¸ì¥ë§Œ
            key_sents = [sentences[idx] for idx in top_indices]
            context_parts.append(" ".join(key_sents))
        return " ".join(context_parts)

    def _build_prompt(self, query: str, context: str) -> str:
        # ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ - ë” êµ¬ì²´ì ì´ê³  ì§€ì‹œì 
        return f"""### ì§€ì‹œì‚¬í•­:
ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ì°¸ê³  ë¬¸ì„œì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ë‹µë³€ì€ ì§ˆë¬¸ì˜ í•µì‹¬ì— ì§‘ì¤‘í•˜ê³ , ì°¸ê³  ë¬¸ì„œì˜ ë‚´ìš©ì„ ì¶©ì‹¤íˆ ë°˜ì˜í•´ì•¼ í•©ë‹ˆë‹¤.
ë¶ˆí•„ìš”í•œ ì •ë³´ë‚˜ ê´€ë ¨ ì—†ëŠ” ë‚´ìš©ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

### ì§ˆë¬¸:
{query}

### ì°¸ê³  ë¬¸ì„œ:
{context}

### ë‹µë³€:
"""


class SimpleLLMSystem:
    """ê°œì„ ëœ ê·œì¹™ ê¸°ë°˜ ë‹µë³€ ì‹œìŠ¤í…œ (LLM ì—†ì´)"""
    
    def __init__(self):
        pass
    
    def generate_answer(self, query: str, context_docs: List[Dict]) -> str:
        """
        ë¬¸ì¥ ì¶”ì¶œ ê¸°ë°˜ ë‹µë³€ ìƒì„± (LLM ëŒ€ì‹  ì‚¬ìš©)
        """
        if not context_docs:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        try:
            # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
            from sentence_transformers import SentenceTransformer, util
            embedding_model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
            
            # ì§ˆë¬¸ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì¥ë“¤ ì¶”ì¶œ
            relevant_sentences = []
            
            for doc in context_docs[:2]:  # ìµœëŒ€ 2ê°œ ë¬¸ì„œ
                doc_text = doc['text']
                sentences = [s.strip() for s in doc_text.split('.') if s.strip() and len(s.strip()) > 10]
                
                if not sentences:
                    continue
                
                # ì§ˆë¬¸ê³¼ ë¬¸ì¥ë“¤ì˜ ìœ ì‚¬ë„ ê³„ì‚°
                q_emb = embedding_model.encode(query)
                s_embs = embedding_model.encode(sentences)
                sims = util.cos_sim(q_emb, s_embs)[0]
                
                # ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì¥ 2ê°œ ì„ íƒ
                top_indices = sims.argsort(descending=True)[:2]
                for idx in top_indices:
                    if sims[idx] > 0.3:  # ìœ ì‚¬ë„ ì„ê³„ê°’
                        relevant_sentences.append(sentences[idx])
            
            if not relevant_sentences:
                return "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            # ë‹µë³€ êµ¬ì„±
            answer = f"ì§ˆë¬¸: {query}\n\n"
            answer += "ê´€ë ¨ ì •ë³´:\n"
            for i, sent in enumerate(relevant_sentences[:3], 1):
                answer += f"{i}. {sent}\n"
            
            return answer
            
        except Exception as e:
            print(f"SimpleLLMSystem ì˜¤ë¥˜: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def _generate_period_answer(self, query: str, docs: List[Dict]) -> str:
        """ê¸°ê°„ ê´€ë ¨ ì§ˆë¬¸ ë‹µë³€"""
        answer_parts = [f"ì§ˆë¬¸: {query}\n\n"]
        
        for i, doc in enumerate(docs, 1):
            doc_text = doc['text']
            metadata = doc['metadata']
            
            # ê¸°ê°„ ê´€ë ¨ ì •ë³´ ì¶”ì¶œ
            if any(keyword in doc_text for keyword in ['ê¸°ê°„', 'ì¼', 'ê°œì›”', 'ë…„', 'ì£¼']):
                answer_parts.append(f"ğŸ“… ë¬¸ì„œ {i}ì—ì„œ ì°¾ì€ ê¸°ê°„ ì •ë³´:")
                answer_parts.append(f"{doc_text}")
                answer_parts.append(f"ì¶œì²˜: {metadata.get('doc_type', 'ë²•ë¥  ë¬¸ì„œ')}")
                answer_parts.append("")
        
        if len(answer_parts) == 1:  # ê¸°ê°„ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš°
            answer_parts.append("ì œê³µëœ ë¬¸ì„œì—ì„œ êµ¬ì²´ì ì¸ ê¸°ê°„ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            answer_parts.append("ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œê±°ë‚˜ ë‹¤ë¥¸ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•´ë³´ì„¸ìš”.")
        
        return "\n".join(answer_parts)
    
    def _generate_condition_answer(self, query: str, docs: List[Dict]) -> str:
        """ì¡°ê±´ ê´€ë ¨ ì§ˆë¬¸ ë‹µë³€"""
        answer_parts = [f"ì§ˆë¬¸: {query}\n\n"]
        
        for i, doc in enumerate(docs, 1):
            doc_text = doc['text']
            metadata = doc['metadata']
            
            answer_parts.append(f"ğŸ“‹ ë¬¸ì„œ {i}ì—ì„œ ì°¾ì€ ì¡°ê±´:")
            answer_parts.append(f"{doc_text}")
            answer_parts.append(f"ì¶œì²˜: {metadata.get('doc_type', 'ë²•ë¥  ë¬¸ì„œ')}")
            answer_parts.append("")
        
        return "\n".join(answer_parts)
    
    def _generate_obligation_answer(self, query: str, docs: List[Dict]) -> str:
        """ì˜ë¬´/ì±…ì„ ê´€ë ¨ ì§ˆë¬¸ ë‹µë³€"""
        answer_parts = [f"ì§ˆë¬¸: {query}\n\n"]
        
        for i, doc in enumerate(docs, 1):
            doc_text = doc['text']
            metadata = doc['metadata']
            
            answer_parts.append(f"âš–ï¸ ë¬¸ì„œ {i}ì—ì„œ ì°¾ì€ ì˜ë¬´/ì±…ì„:")
            answer_parts.append(f"{doc_text}")
            answer_parts.append(f"ì¶œì²˜: {metadata.get('doc_type', 'ë²•ë¥  ë¬¸ì„œ')}")
            answer_parts.append("")
        
        return "\n".join(answer_parts)
    
    def _generate_general_answer(self, query: str, docs: List[Dict]) -> str:
        """ì¼ë°˜ì ì¸ ì§ˆë¬¸ ë‹µë³€"""
        answer_parts = [f"ì§ˆë¬¸: {query}\n\n"]
        
        for i, doc in enumerate(docs, 1):
            doc_text = doc['text']
            metadata = doc['metadata']
            
            answer_parts.append(f"ğŸ“„ ë¬¸ì„œ {i}ì—ì„œ ì°¾ì€ ì •ë³´:")
            answer_parts.append(f"{doc_text}")
            answer_parts.append(f"ì¶œì²˜: {metadata.get('doc_type', 'ë²•ë¥  ë¬¸ì„œ')}")
            answer_parts.append("")
        
        return "\n".join(answer_parts)


def test_llm_system():
    """LLM ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    print("ğŸ¤– LLM ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    # ê°„ë‹¨í•œ ì‹œìŠ¤í…œìœ¼ë¡œ í…ŒìŠ¤íŠ¸ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)
    llm_system = SimpleLLMSystem()
    
    # í…ŒìŠ¤íŠ¸ìš© ì»¨í…ìŠ¤íŠ¸
    test_context = [
        {
            'text': 'ê³„ì•½ì„œì—ëŠ” ë‹¹ì‚¬ìì˜ ê¸°ë³¸ ì •ë³´, ê³„ì•½ ëª©ì , ê³„ì•½ ê¸°ê°„, ê³„ì•½ ì¡°ê±´ ë“±ì´ ëª…ì‹œë˜ì–´ì•¼ í•©ë‹ˆë‹¤.',
            'metadata': {
                'doc_type': 'ê³„ì•½ì„œ',
                'title': 'ê³„ì•½ì„œ ê¸°ë³¸ ì¡°í•­'
            }
        }
    ]
    
    test_query = "ê³„ì•½ì„œì—ëŠ” ì–´ë–¤ ë‚´ìš©ì´ í¬í•¨ë˜ì–´ì•¼ í•˜ë‚˜ìš”?"
    
    answer = llm_system.generate_answer(test_query, test_context)
    print(f"ì§ˆë¬¸: {test_query}")
    print(f"ë‹µë³€: {answer}")


if __name__ == "__main__":
    test_llm_system() 