import os
from typing import List, Dict, Optional
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch


class LLMSystem:
    """ë¡œì»¬ LLM ê¸°ë°˜ ë‹µë³€ ìƒì„± ì‹œìŠ¤í…œ"""
    
    def __init__(self, model_name: str = "microsoft/DialoGPT-medium"):
        """
        LLM ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        
        Args:
            model_name: ì‚¬ìš©í•  ëª¨ë¸ëª… (CPU í˜¸í™˜ ëª¨ë¸ ê¶Œì¥)
        """
        self.model_name = model_name
        self.device = "cpu"  # MacBook CPU ì‚¬ìš©
        
        print(f"ğŸ¤– LLM ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float32,  # CPU í˜¸í™˜
            low_cpu_mem_usage=True
        )
        
        # íŒ¨ë”© í† í° ì„¤ì •
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
        print("âœ… LLM ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
    
    def generate_answer(self, query: str, context_docs: List[Dict], max_length: int = 512) -> str:
        """
        ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹µë³€ ìƒì„±
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            context_docs: ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œë“¤
            max_length: ìµœëŒ€ ìƒì„± ê¸¸ì´
            
        Returns:
            ìƒì„±ëœ ë‹µë³€
        """
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context_text = self._build_context(context_docs)
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = self._build_prompt(query, context_text)
        
        # í† í°í™”
        inputs = self.tokenizer.encode(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=max_length
        )
        
        # ë‹µë³€ ìƒì„±
        with torch.no_grad():
            outputs = self.model.generate(
                inputs,
                max_length=max_length,
                num_return_sequences=1,
                temperature=0.7,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # ë””ì½”ë”©
        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # í”„ë¡¬í”„íŠ¸ ì œê±°í•˜ê³  ë‹µë³€ë§Œ ì¶”ì¶œ
        answer = generated_text[len(prompt):].strip()
        
        return answer
    
    def _build_context(self, context_docs: List[Dict]) -> str:
        """ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±"""
        context_parts = []
        
        for i, doc in enumerate(context_docs, 1):
            doc_text = doc['text']
            metadata = doc['metadata']
            
            context_part = f"[ë¬¸ì„œ {i}] {doc_text}"
            if metadata.get('title'):
                context_part = f"[ë¬¸ì„œ {i} - {metadata['title']}] {doc_text}"
            
            context_parts.append(context_part)
        
        return "\n\n".join(context_parts)
    
    def _build_prompt(self, query: str, context: str) -> str:
        """RAG í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        prompt = f"""ë‹¤ìŒì€ ë²•ë¥  ë¬¸ì„œ ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸° ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.

ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {query}

ìœ„ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”. ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.

ë‹µë³€:"""
        
        return prompt


class SimpleLLMSystem:
    """ê°„ë‹¨í•œ í…œí”Œë¦¿ ê¸°ë°˜ ë‹µë³€ ì‹œìŠ¤í…œ (LLM ì—†ì´)"""
    
    def __init__(self):
        pass
    
    def generate_answer(self, query: str, context_docs: List[Dict]) -> str:
        """
        í…œí”Œë¦¿ ê¸°ë°˜ ë‹µë³€ ìƒì„± (LLM ì—†ì´)
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            context_docs: ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œë“¤
            
        Returns:
            ìƒì„±ëœ ë‹µë³€
        """
        if not context_docs:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œ ì„ íƒ
        best_doc = context_docs[0]
        
        # ê°„ë‹¨í•œ í…œí”Œë¦¿ ê¸°ë°˜ ë‹µë³€
        answer = f"""ì§ˆë¬¸: {query}

ê´€ë ¨ ë¬¸ì„œì—ì„œ ì°¾ì€ ë‚´ìš©:
{best_doc['text']}

ì´ ë¬¸ì„œëŠ” {best_doc['metadata'].get('doc_type', 'ë²•ë¥  ë¬¸ì„œ')}ì— ì†í•˜ë©°, 
ì œëª©ì€ "{best_doc['metadata'].get('title', 'ì œëª© ì—†ìŒ')}"ì…ë‹ˆë‹¤.

ë” ìì„¸í•œ ì •ë³´ê°€ í•„ìš”í•˜ì‹œë©´ ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”."""
        
        return answer


def test_llm_system():
    """LLM ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    print("ğŸ¤– LLM ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    # ê°„ë‹¨í•œ ì‹œìŠ¤í…œìœ¼ë¡œ í…ŒìŠ¤íŠ¸ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)
    llm_system = SimpleLLMSystem()
    
    # í…ŒìŠ¤íŠ¸ìš© ì»¨í…ìŠ¤íŠ¸
    test_context = [
        {
            'text': 'ê³„ì•½ì„œì—ëŠ” ë‹¹ì‚¬ìì˜ ê¸°ë³¸ ì •ë³´, ê³„ì•½ ëª©ì , ê³„ì•½ ê¸°ê°„, ê³„ì•½ ì¡°ê±´ ë“±ì´ ëª…ì‹œë˜ì–´ì•¼ í•©ë‹ˆë‹¤.',
            'metadata': {
                'doc_type': 'ê³„ì•½ì„œ',
                'title': 'ê³„ì•½ì„œ ê¸°ë³¸ ì¡°í•­'
            }
        }
    ]
    
    test_query = "ê³„ì•½ì„œì—ëŠ” ì–´ë–¤ ë‚´ìš©ì´ í¬í•¨ë˜ì–´ì•¼ í•˜ë‚˜ìš”?"
    
    answer = llm_system.generate_answer(test_query, test_context)
    print(f"ì§ˆë¬¸: {test_query}")
    print(f"ë‹µë³€: {answer}")


if __name__ == "__main__":
    test_llm_system() 