import json
import os
import time
from typing import List, Dict, Any
from datetime import datetime
import numpy as np
import chromadb

# í‘œì¤€ í‰ê°€ ì§€í‘œë“¤ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
    from rouge_score import rouge_scorer
    from nltk.translate.meteor_score import meteor_score
    import nltk
    # NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        nltk.download('punkt')
    try:
        nltk.data.find('corpora/wordnet')
    except LookupError:
        nltk.download('wordnet')
    try:
        nltk.data.find('corpora/omw-1.4')
    except LookupError:
        nltk.download('omw-1.4')
except ImportError as e:
    print(f"âš ï¸ ì¼ë¶€ í‰ê°€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ í•„ìš”: {e}")
    print("pip install nltk rouge-score")

class RAGEvaluator:
    """RAG ì‹œìŠ¤í…œ ì„±ëŠ¥ í‰ê°€ê¸° (í‘œì¤€ ì§€í‘œë§Œ ì‚¬ìš©)"""

    def __init__(self, db_path: str = "data/embeddings/complete_db"):
        self.db_path = db_path
        self.client = chromadb.PersistentClient(path=db_path)
        self.collection = self.client.get_collection("complete_knowledge_qa")
        
        # í‘œì¤€ í‰ê°€ ì§€í‘œ ì´ˆê¸°í™”
        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
        self.smooth = SmoothingFunction().method1

    def load_validation_data(self) -> List[Dict]:
        """Validation ë°ì´í„° ë¡œë“œ (í‰ê°€ìš© ì§ˆë¬¸-ë‹µë³€ ìŒ)"""
        with open("data/processed/validation_qa.json", "r", encoding="utf-8") as f:
            validation_data = json.load(f)
        return validation_data

    def search_similar_docs(self, query: str, n_results: int = 3) -> List[Dict]:
        """Training DBì—ì„œ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰"""
        from sentence_transformers import SentenceTransformer
        embedding_model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
        query_embedding = embedding_model.encode(query).tolist()

        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=n_results
        )

        formatted_results = []
        for i in range(len(results['documents'][0])):
            result = {
                'text': results['documents'][0][i],
                'metadata': results['metadatas'][0][i],
                'id': results['ids'][0][i],
                'distance': results['distances'][0][i] if 'distances' in results else None
            }
            formatted_results.append(result)

        return formatted_results

    def calculate_bleu_score(self, reference: str, candidate: str) -> float:
        """BLEU ì ìˆ˜ ê³„ì‚°"""
        try:
            reference_tokens = reference.split()
            candidate_tokens = candidate.split()
            return sentence_bleu([reference_tokens], candidate_tokens, smoothing_function=self.smooth)
        except:
            return 0.0

    def calculate_rouge_scores(self, reference: str, candidate: str) -> Dict[str, float]:
        """ROUGE ì ìˆ˜ ê³„ì‚°"""
        try:
            scores = self.rouge_scorer.score(reference, candidate)
            return {
                'rouge1': scores['rouge1'].fmeasure,
                'rouge2': scores['rouge2'].fmeasure,
                'rougeL': scores['rougeL'].fmeasure
            }
        except:
            return {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}

    def calculate_meteor_score(self, reference: str, candidate: str) -> float:
        """METEOR ì ìˆ˜ ê³„ì‚°"""
        try:
            reference_tokens = reference.split()
            candidate_tokens = candidate.split()
            return meteor_score([reference_tokens], candidate_tokens)
        except:
            return 0.0

    def evaluate_single_qa(self, qa_pair: Dict, generated_answer: str, retrieved_docs: List[Dict]) -> Dict[str, Any]:
        """ë‹¨ì¼ Q&A ìŒ í‰ê°€ (í‘œì¤€ ì§€í‘œë§Œ ì‚¬ìš©)"""
        question = qa_pair["question"]
        ground_truth_answer = qa_pair["answer"]

        # í‘œì¤€ í‰ê°€ ì§€í‘œ ê³„ì‚°
        bleu_score = self.calculate_bleu_score(ground_truth_answer, generated_answer)
        rouge_scores = self.calculate_rouge_scores(ground_truth_answer, generated_answer)
        meteor_score_val = self.calculate_meteor_score(ground_truth_answer, generated_answer)

        # ì¢…í•© ì ìˆ˜ (í‘œì¤€ ì§€í‘œë“¤ì˜ í‰ê· )
        comprehensive_score = np.mean([
            bleu_score,
            rouge_scores['rouge1'],
            rouge_scores['rouge2'],
            rouge_scores['rougeL'],
            meteor_score_val
        ])

        return {
            "question": question,
            "ground_truth": ground_truth_answer,
            "generated_answer": generated_answer,
            "retrieved_docs_count": len(retrieved_docs),
            
            # í‘œì¤€ í‰ê°€ ì§€í‘œ
            "bleu_score": bleu_score,
            "rouge1_score": rouge_scores['rouge1'],
            "rouge2_score": rouge_scores['rouge2'],
            "rougeL_score": rouge_scores['rougeL'],
            "meteor_score": meteor_score_val,
            "comprehensive_score": comprehensive_score
        }

    def evaluate_rag_system(self, llm_system, sample_size: int = 50) -> Dict[str, Any]:
        """RAG ì‹œìŠ¤í…œ ì „ì²´ í‰ê°€ (í‘œì¤€ ì§€í‘œë§Œ ì‚¬ìš©)"""
        print("ğŸ§ª RAG ì‹œìŠ¤í…œ í‰ê°€ ì‹œì‘...")
        print("ğŸ“‹ í‰ê°€ ë°©ì‹:")
        print("  1. Training ë°ì´í„°ë¡œ ë²¡í„° DB êµ¬ì¶• (ì™„ë£Œ)")
        print("  2. Validation ë°ì´í„°ì˜ ì§ˆë¬¸ìœ¼ë¡œ ê²€ìƒ‰")
        print("  3. ê²€ìƒ‰ëœ ë¬¸ì„œ + ì§ˆë¬¸ â†’ LLM ë‹µë³€ ìƒì„±")
        print("  4. ìƒì„±ëœ ë‹µë³€ vs Validation ì •ë‹µ ë¹„êµ")
        print("  5. í‘œì¤€ í‰ê°€ ì§€í‘œ (BLEU, ROUGE, METEOR) ê³„ì‚°")

        # Validation ë°ì´í„° ë¡œë“œ
        validation_data = self.load_validation_data()
        print(f"ğŸ“Š Validation ë°ì´í„°: {len(validation_data)}ê°œ")

        # ìƒ˜í”Œë§ (ì „ì²´ í‰ê°€ëŠ” ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼)
        if sample_size and sample_size < len(validation_data):
            import random
            random.seed(42)  # ì¬í˜„ ê°€ëŠ¥ì„±ì„ ìœ„í•´
            validation_sample = random.sample(validation_data, sample_size)
        else:
            validation_sample = validation_data

        print(f"ğŸ” í‰ê°€í•  ìƒ˜í”Œ: {len(validation_sample)}ê°œ")

        # í‰ê°€ ì‹¤í–‰
        evaluation_results = []
        total_time = 0

        for i, qa_pair in enumerate(validation_sample):
            print(f"í‰ê°€ ì§„í–‰ ì¤‘... ({i+1}/{len(validation_sample)})")
            
            # ë‚¨ì€ ì‹œê°„ ê³„ì‚°
            if i > 0:
                avg_time_per_sample = total_time / i
                remaining_samples = len(validation_sample) - i
                estimated_remaining_time = avg_time_per_sample * remaining_samples
                print(f"   ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {estimated_remaining_time/60:.1f}ë¶„")
            
            start_time = time.time()

            # 1. Validation ì§ˆë¬¸ìœ¼ë¡œ Training DBì—ì„œ ê²€ìƒ‰
            retrieved_docs = self.search_similar_docs(qa_pair["question"], n_results=3)

            # 2. ê²€ìƒ‰ëœ ë¬¸ì„œ + ì§ˆë¬¸ìœ¼ë¡œ LLM ë‹µë³€ ìƒì„±
            try:
                generated_answer = llm_system.generate_answer(qa_pair["question"], retrieved_docs)
            except Exception as e:
                print(f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
                generated_answer = "ë‹µë³€ ìƒì„± ì‹¤íŒ¨"

            generation_time = time.time() - start_time
            total_time += generation_time

            # 3. í‰ê°€ (í‘œì¤€ ì§€í‘œë§Œ ì‚¬ìš©)
            eval_result = self.evaluate_single_qa(qa_pair, generated_answer, retrieved_docs)
            eval_result["generation_time"] = generation_time
            evaluation_results.append(eval_result)

        # ì „ì²´ í†µê³„ ê³„ì‚°
        bleu_scores = [r["bleu_score"] for r in evaluation_results]
        rouge1_scores = [r["rouge1_score"] for r in evaluation_results]
        rouge2_scores = [r["rouge2_score"] for r in evaluation_results]
        rougeL_scores = [r["rougeL_score"] for r in evaluation_results]
        meteor_scores = [r["meteor_score"] for r in evaluation_results]
        comprehensive_scores = [r["comprehensive_score"] for r in evaluation_results]
        generation_times = [r["generation_time"] for r in evaluation_results]

        # ì„±ê³µë¥  ê³„ì‚° (ë‹µë³€ ìƒì„± ì‹¤íŒ¨ ì œì™¸)
        successful_generations = [r for r in evaluation_results if r["generated_answer"] != "ë‹µë³€ ìƒì„± ì‹¤íŒ¨"]
        success_rate = len(successful_generations) / len(evaluation_results)

        overall_results = {
            "total_samples": len(evaluation_results),
            "successful_generations": len(successful_generations),
            "success_rate": success_rate,
            
            # í‘œì¤€ í‰ê°€ ì§€í‘œ
            "avg_bleu_score": np.mean(bleu_scores),
            "avg_rouge1_score": np.mean(rouge1_scores),
            "avg_rouge2_score": np.mean(rouge2_scores),
            "avg_rougeL_score": np.mean(rougeL_scores),
            "avg_meteor_score": np.mean(meteor_scores),
            "avg_comprehensive_score": np.mean(comprehensive_scores),
            
            # ì„±ëŠ¥ ì§€í‘œ
            "avg_generation_time": np.mean(generation_times),
            "total_evaluation_time": total_time,
            "detailed_results": evaluation_results
        }

        return overall_results

    def save_evaluation_results(self, results: Dict, model_name: str):
        """í‰ê°€ ê²°ê³¼ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"experiments/evaluation_results_{model_name}_{timestamp}.json"

        os.makedirs("experiments", exist_ok=True)

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ í‰ê°€ ê²°ê³¼ ì €ì¥: {filename}")
        return filename

    def print_evaluation_summary(self, results: Dict, model_name: str):
        """í‰ê°€ ê²°ê³¼ ìš”ì•½ ì¶œë ¥ (í‘œì¤€ ì§€í‘œë§Œ)"""
        print(f"\nğŸ“Š {model_name} í‰ê°€ ê²°ê³¼ ìš”ì•½")
        print("=" * 50)
        print(f"ì´ í‰ê°€ ìƒ˜í”Œ: {results['total_samples']}ê°œ")
        print(f"ì„±ê³µì  ë‹µë³€ ìƒì„±: {results['successful_generations']}ê°œ")
        print(f"ì„±ê³µë¥ : {results['success_rate']:.2%}")
        print()
        print("ğŸ“ˆ í‘œì¤€ í‰ê°€ ì§€í‘œ:")
        print(f"  BLEU ì ìˆ˜: {results['avg_bleu_score']:.3f}")
        print(f"  ROUGE-1: {results['avg_rouge1_score']:.3f}")
        print(f"  ROUGE-2: {results['avg_rouge2_score']:.3f}")
        print(f"  ROUGE-L: {results['avg_rougeL_score']:.3f}")
        print(f"  METEOR: {results['avg_meteor_score']:.3f}")
        print(f"  ì¢…í•© ì ìˆ˜: {results['avg_comprehensive_score']:.3f}")
        print()
        print(f"â±ï¸ ì„±ëŠ¥:")
        print(f"  í‰ê·  ìƒì„± ì‹œê°„: {results['avg_generation_time']:.2f}ì´ˆ")
        print(f"  ì´ í‰ê°€ ì‹œê°„: {results['total_evaluation_time']:.2f}ì´ˆ")
        print("=" * 50)

def main():
    """í‰ê°€ ì‹¤í–‰ ì˜ˆì‹œ"""
    import sys
    import os
    
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
    sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
    
    evaluator = RAGEvaluator()

    # LLM ì‹œìŠ¤í…œ ì„í¬íŠ¸
    from src.generation.llm_system import LLMSystem

    # ì—¬ëŸ¬ ëª¨ë¸ í…ŒìŠ¤íŠ¸
    models_to_test = [
        "EleutherAI/polyglot-ko-1.3b",
        "skt/kogpt2-base-v2",
        "beomi/gemma-ko-2b"
    ]

    for model_name in models_to_test:
        print(f"\nğŸ¤– {model_name} ëª¨ë¸ í‰ê°€ ì‹œì‘...")

        try:
            llm_system = LLMSystem(model_name)
            results = evaluator.evaluate_rag_system(llm_system, sample_size=20)
            evaluator.save_evaluation_results(results, model_name.replace("/", "_"))
            evaluator.print_evaluation_summary(results, model_name)
        except Exception as e:
            print(f"âŒ {model_name} ëª¨ë¸ í‰ê°€ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    main() 