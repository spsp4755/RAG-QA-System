import json
import os
import time
from typing import List, Dict, Any
from datetime import datetime
import numpy as np
from sentence_transformers import SentenceTransformer, util
import chromadb

class RAGEvaluator:
    """RAG ì‹œìŠ¤í…œ ì„±ëŠ¥ í‰ê°€ê¸°"""
    
    def __init__(self, training_db_path: str = "data/embeddings/training_db"):
        self.training_db_path = training_db_path
        self.client = chromadb.PersistentClient(path=training_db_path)
        self.collection = self.client.get_collection("training_knowledge_qa")
        self.embedding_model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
        
    def load_validation_data(self) -> List[Dict]:
        """Validation ë°ì´í„° ë¡œë“œ (í‰ê°€ìš© ì§ˆë¬¸-ë‹µë³€ ìŒ)"""
        with open("data/processed/validation_qa.json", "r", encoding="utf-8") as f:
            validation_data = json.load(f)
        return validation_data
    
    def search_similar_docs(self, query: str, n_results: int = 3) -> List[Dict]:
        """Training DBì—ì„œ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰"""
        query_embedding = self.embedding_model.encode(query).tolist()
        
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=n_results
        )
        
        formatted_results = []
        for i in range(len(results['documents'][0])):
            result = {
                'text': results['documents'][0][i],
                'metadata': results['metadatas'][0][i],
                'id': results['ids'][0][i],
                'distance': results['distances'][0][i] if 'distances' in results else None
            }
            formatted_results.append(result)
            
        return formatted_results
    
    def calculate_semantic_similarity(self, text1: str, text2: str) -> float:
        """ë‘ í…ìŠ¤íŠ¸ ê°„ì˜ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°"""
        emb1 = self.embedding_model.encode(text1)
        emb2 = self.embedding_model.encode(text2)
        similarity = util.cos_sim(emb1, emb2)[0][0].item()
        return similarity
    
    def evaluate_single_qa(self, qa_pair: Dict, generated_answer: str, retrieved_docs: List[Dict]) -> Dict[str, Any]:
        """ë‹¨ì¼ Q&A ìŒ í‰ê°€"""
        question = qa_pair["question"]
        ground_truth_answer = qa_pair["answer"]
        
        # 1. ì˜ë¯¸ì  ìœ ì‚¬ë„ (Semantic Similarity)
        semantic_similarity = self.calculate_semantic_similarity(
            generated_answer, ground_truth_answer
        )
        
        # 2. ë‹µë³€ ê¸¸ì´ ë¹„ìœ¨
        length_ratio = len(generated_answer) / len(ground_truth_answer) if len(ground_truth_answer) > 0 else 0
        
        # 3. í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ (ê°„ë‹¨í•œ ë²„ì „)
        gt_words = set(ground_truth_answer.lower().split())
        gen_words = set(generated_answer.lower().split())
        keyword_overlap = len(gt_words.intersection(gen_words)) / len(gt_words) if len(gt_words) > 0 else 0
        
        # 4. ê²€ìƒ‰ í’ˆì§ˆ í‰ê°€ (ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì§ˆë¬¸ê³¼ ê´€ë ¨ìˆëŠ”ì§€)
        search_relevance = 0
        if retrieved_docs:
            # ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ê³¼ ì§ˆë¬¸ì˜ ìœ ì‚¬ë„ í‰ê· 
            doc_similarities = []
            for doc in retrieved_docs:
                doc_sim = self.calculate_semantic_similarity(question, doc['text'])
                doc_similarities.append(doc_sim)
            search_relevance = np.mean(doc_similarities)
        
        return {
            "question": question,
            "ground_truth": ground_truth_answer,
            "generated_answer": generated_answer,
            "retrieved_docs_count": len(retrieved_docs),
            "semantic_similarity": semantic_similarity,
            "length_ratio": length_ratio,
            "keyword_overlap": keyword_overlap,
            "search_relevance": search_relevance,
            "evaluation_score": (semantic_similarity + keyword_overlap + search_relevance) / 3  # ì¢…í•© ì ìˆ˜
        }
    
    def evaluate_rag_system(self, llm_system, sample_size: int = 50) -> Dict[str, Any]:
        """RAG ì‹œìŠ¤í…œ ì „ì²´ í‰ê°€"""
        print("ğŸ§ª RAG ì‹œìŠ¤í…œ í‰ê°€ ì‹œì‘...")
        print("ğŸ“‹ í‰ê°€ ë°©ì‹:")
        print("  1. Training ë°ì´í„°ë¡œ ë²¡í„° DB êµ¬ì¶• (ì™„ë£Œ)")
        print("  2. Validation ë°ì´í„°ì˜ ì§ˆë¬¸ìœ¼ë¡œ ê²€ìƒ‰")
        print("  3. ê²€ìƒ‰ëœ ë¬¸ì„œ + ì§ˆë¬¸ â†’ LLM ë‹µë³€ ìƒì„±")
        print("  4. ìƒì„±ëœ ë‹µë³€ vs Validation ì •ë‹µ ë¹„êµ")
        
        # Validation ë°ì´í„° ë¡œë“œ
        validation_data = self.load_validation_data()
        print(f"ğŸ“Š Validation ë°ì´í„°: {len(validation_data)}ê°œ")
        
        # ìƒ˜í”Œë§ (ì „ì²´ í‰ê°€ëŠ” ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼)
        if sample_size and sample_size < len(validation_data):
            import random
            random.seed(42)  # ì¬í˜„ ê°€ëŠ¥ì„±ì„ ìœ„í•´
            validation_sample = random.sample(validation_data, sample_size)
        else:
            validation_sample = validation_data
        
        print(f"ğŸ” í‰ê°€í•  ìƒ˜í”Œ: {len(validation_sample)}ê°œ")
        
        # í‰ê°€ ì‹¤í–‰
        evaluation_results = []
        total_time = 0
        
        for i, qa_pair in enumerate(validation_sample):
            print(f"í‰ê°€ ì§„í–‰ ì¤‘... ({i+1}/{len(validation_sample)})")
            
            start_time = time.time()
            
            # 1. Validation ì§ˆë¬¸ìœ¼ë¡œ Training DBì—ì„œ ê²€ìƒ‰
            retrieved_docs = self.search_similar_docs(qa_pair["question"], n_results=3)
            
            # 2. ê²€ìƒ‰ëœ ë¬¸ì„œ + ì§ˆë¬¸ìœ¼ë¡œ LLM ë‹µë³€ ìƒì„±
            try:
                generated_answer = llm_system.generate_answer(qa_pair["question"], retrieved_docs)
            except Exception as e:
                print(f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
                generated_answer = "ë‹µë³€ ìƒì„± ì‹¤íŒ¨"
            
            generation_time = time.time() - start_time
            total_time += generation_time
            
            # 3. í‰ê°€
            eval_result = self.evaluate_single_qa(qa_pair, generated_answer, retrieved_docs)
            eval_result["generation_time"] = generation_time
            evaluation_results.append(eval_result)
        
        # ì „ì²´ í†µê³„ ê³„ì‚°
        semantic_similarities = [r["semantic_similarity"] for r in evaluation_results]
        length_ratios = [r["length_ratio"] for r in evaluation_results]
        keyword_overlaps = [r["keyword_overlap"] for r in evaluation_results]
        search_relevances = [r["search_relevance"] for r in evaluation_results]
        evaluation_scores = [r["evaluation_score"] for r in evaluation_results]
        generation_times = [r["generation_time"] for r in evaluation_results]
        
        # ì„±ê³µë¥  ê³„ì‚° (ë‹µë³€ ìƒì„± ì‹¤íŒ¨ ì œì™¸)
        successful_generations = [r for r in evaluation_results if r["generated_answer"] != "ë‹µë³€ ìƒì„± ì‹¤íŒ¨"]
        success_rate = len(successful_generations) / len(evaluation_results)
        
        overall_results = {
            "total_samples": len(evaluation_results),
            "successful_generations": len(successful_generations),
            "success_rate": success_rate,
            "avg_semantic_similarity": np.mean(semantic_similarities),
            "avg_length_ratio": np.mean(length_ratios),
            "avg_keyword_overlap": np.mean(keyword_overlaps),
            "avg_search_relevance": np.mean(search_relevances),
            "avg_evaluation_score": np.mean(evaluation_scores),
            "avg_generation_time": np.mean(generation_times),
            "total_evaluation_time": total_time,
            "detailed_results": evaluation_results
        }
        
        return overall_results
    
    def save_evaluation_results(self, results: Dict, model_name: str):
        """í‰ê°€ ê²°ê³¼ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"experiments/evaluation_results_{model_name}_{timestamp}.json"
        
        os.makedirs("experiments", exist_ok=True)
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ í‰ê°€ ê²°ê³¼ ì €ì¥: {filename}")
        return filename
    
    def print_evaluation_summary(self, results: Dict, model_name: str):
        """í‰ê°€ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print(f"\nğŸ“Š {model_name} í‰ê°€ ê²°ê³¼ ìš”ì•½")
        print("=" * 50)
        print(f"ì´ í‰ê°€ ìƒ˜í”Œ: {results['total_samples']}ê°œ")
        print(f"ì„±ê³µì  ë‹µë³€ ìƒì„±: {results['successful_generations']}ê°œ")
        print(f"ì„±ê³µë¥ : {results['success_rate']:.2%}")
        print(f"í‰ê·  ì˜ë¯¸ì  ìœ ì‚¬ë„: {results['avg_semantic_similarity']:.3f}")
        print(f"í‰ê·  í‚¤ì›Œë“œ ì¤‘ë³µë„: {results['avg_keyword_overlap']:.3f}")
        print(f"í‰ê·  ê²€ìƒ‰ ê´€ë ¨ì„±: {results['avg_search_relevance']:.3f}")
        print(f"í‰ê·  ì¢…í•© ì ìˆ˜: {results['avg_evaluation_score']:.3f}")
        print(f"í‰ê·  ìƒì„± ì‹œê°„: {results['avg_generation_time']:.2f}ì´ˆ")
        print(f"ì´ í‰ê°€ ì‹œê°„: {results['total_evaluation_time']:.2f}ì´ˆ")
        print("=" * 50)

def main():
    """í‰ê°€ ì‹¤í–‰ ì˜ˆì‹œ"""
    evaluator = RAGEvaluator()
    
    # LLM ì‹œìŠ¤í…œ ì„í¬íŠ¸
    from src.generation.llm_system import LLMSystem
    
    # ì—¬ëŸ¬ ëª¨ë¸ í…ŒìŠ¤íŠ¸
    models_to_test = [
        "EleutherAI/polyglot-ko-1.3b",
        "skt/kogpt2-base-v2"
    ]
    
    for model_name in models_to_test:
        print(f"\nğŸ¤– {model_name} ëª¨ë¸ í‰ê°€ ì‹œì‘...")
        
        try:
            llm_system = LLMSystem(model_name)
            results = evaluator.evaluate_rag_system(llm_system, sample_size=20)
            evaluator.save_evaluation_results(results, model_name.replace("/", "_"))
            evaluator.print_evaluation_summary(results, model_name)
        except Exception as e:
            print(f"âŒ {model_name} ëª¨ë¸ í‰ê°€ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    main() 