import json
import os
import time
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
import chromadb

def build_mpnet_vector_db():
    """ê³ ì„±ëŠ¥ ë‹¤êµ­ì–´ MPNet ëª¨ë¸ë¡œ ë²¡í„° DB êµ¬ì¶•"""
    print("ğŸ—ï¸ ê³ ì„±ëŠ¥ ë‹¤êµ­ì–´ MPNet ëª¨ë¸ë¡œ ë²¡í„° DB êµ¬ì¶• ì‹œì‘...")

    # 1. Training ë°ì´í„°ì™€ Validation ë°ì´í„° ëª¨ë‘ ë¡œë“œ
    with open("data/processed/training_qa.json", "r", encoding="utf-8") as f:
        training_qa = json.load(f)
    
    with open("data/processed/validation_qa.json", "r", encoding="utf-8") as f:
        validation_qa = json.load(f)

    print(f"ğŸ“š Training ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(training_qa)}ê°œ")
    print(f"ğŸ“š Validation ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(validation_qa)}ê°œ")

    # 2. ê³ ì„±ëŠ¥ ë‹¤êµ­ì–´ MPNet ì„ë² ë”© ëª¨ë¸ ì¤€ë¹„
    model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-mpnet-base-v2")
    print("ğŸ”¤ ê³ ì„±ëŠ¥ ë‹¤êµ­ì–´ MPNet ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    # 3. ChromaDB ì¤€ë¹„ (MPNetìš©)
    persist_dir = "data/embeddings/mpnet_db"
    os.makedirs(persist_dir, exist_ok=True)
    client = chromadb.PersistentClient(path=persist_dir)

    # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ (ì¬êµ¬ì¶•)
    try:
        client.delete_collection("mpnet_knowledge_qa")
        print("ğŸ—‘ï¸ ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ ì™„ë£Œ")
    except:
        pass

    collection = client.create_collection("mpnet_knowledge_qa")
    print("ğŸ“¦ ìƒˆ ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ")

    # 4. ë°ì´í„° ì¶”ê°€
    start_time = time.time()
    doc_count = 0

    # Training ë°ì´í„° ì²˜ë¦¬ (context + input/output ëª¨ë‘ í¬í•¨)
    for idx, qa in tqdm(enumerate(training_qa), total=len(training_qa), desc="Training ë°ì´í„° ì„ë² ë”©"):
        # ì»¨í…ìŠ¤íŠ¸ (sentences)
        context = qa["context"]
        if context.strip():  # ë¹ˆ ì»¨í…ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê²½ìš°ë§Œ
            metadata = qa.get("metadata", {}).copy()
            metadata.update({
                "question": qa["question"],
                "answer": qa["answer"],
                "instruction": qa.get("instruction", ""),
                "data_type": "context",  # ì»¨í…ìŠ¤íŠ¸ ë°ì´í„°ì„ì„ í‘œì‹œ
                "source": "mpnet_db",
                "embedding_model": "sentence-transformers/paraphrase-multilingual-mpnet-base-v2",
                "data_split": "training"  # Training ë°ì´í„°ì„ì„ í‘œì‹œ
            })

            # context ì„ë² ë”©
            embedding = model.encode([context])[0]
            collection.add(
                ids=[f"context_{doc_count}"],
                embeddings=[embedding.tolist()],
                documents=[context],
                metadatas=[metadata]
            )
            doc_count += 1

        # Training ë°ì´í„°ì˜ input/outputì„ ë³„ë„ ë¬¸ì„œë¡œ ì¶”ê°€
        question = qa["question"]
        answer = qa["answer"]
        
        qa_text = f"ì§ˆë¬¸: {question}\në‹µë³€: {answer}"
        if qa_text.strip():
            metadata = qa.get("metadata", {}).copy()
            metadata.update({
                "question": question,
                "answer": answer,
                "instruction": qa.get("instruction", ""),
                "data_type": "qa_pair",  # Q&A ìŒ ë°ì´í„°ì„ì„ í‘œì‹œ
                "source": "mpnet_db",
                "embedding_model": "sentence-transformers/paraphrase-multilingual-mpnet-base-v2",
                "data_split": "training"  # Training ë°ì´í„°ì„ì„ í‘œì‹œ
            })

            # Q&A ìŒ ì„ë² ë”©
            embedding = model.encode([qa_text])[0]
            collection.add(
                ids=[f"qa_pair_{doc_count}"],
                embeddings=[embedding.tolist()],
                documents=[qa_text],
                metadatas=[metadata]
            )
            doc_count += 1

    # Validation ë°ì´í„° ì²˜ë¦¬ (contextë§Œ í¬í•¨, input/outputì€ ì œì™¸)
    for idx, qa in tqdm(enumerate(validation_qa), total=len(validation_qa), desc="Validation ë°ì´í„° ì„ë² ë”©"):
        # ì»¨í…ìŠ¤íŠ¸ (sentences)ë§Œ í¬í•¨
        context = qa["context"]
        if context.strip():  # ë¹ˆ ì»¨í…ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê²½ìš°ë§Œ
            metadata = qa.get("metadata", {}).copy()
            metadata.update({
                "question": qa["question"],
                "answer": qa["answer"],
                "instruction": qa.get("instruction", ""),
                "data_type": "context",  # ì»¨í…ìŠ¤íŠ¸ ë°ì´í„°ì„ì„ í‘œì‹œ
                "source": "mpnet_db",
                "embedding_model": "sentence-transformers/paraphrase-multilingual-mpnet-base-v2",
                "data_split": "validation"  # Validation ë°ì´í„°ì„ì„ í‘œì‹œ
            })

            # context ì„ë² ë”©
            embedding = model.encode([context])[0]
            collection.add(
                ids=[f"validation_context_{doc_count}"],
                embeddings=[embedding.tolist()],
                documents=[context],
                metadatas=[metadata]
            )
            doc_count += 1

    total_time = time.time() - start_time
    print(f"âœ… ê³ ì„±ëŠ¥ ë‹¤êµ­ì–´ MPNet ë²¡í„° DB êµ¬ì¶• ì™„ë£Œ!")
    print(f"ğŸ“Š ì´ ë¬¸ì„œ ìˆ˜: {collection.count()}")
    print(f"â±ï¸ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ")
    print(f"ğŸ”¤ ì‚¬ìš© ëª¨ë¸: sentence-transformers/paraphrase-multilingual-mpnet-base-v2")

    return persist_dir

if __name__ == "__main__":
    build_mpnet_vector_db() 